<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>info.puton.component</groupId>
  <artifactId>spark-transfer</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <packaging>jar</packaging>

  <name>spark-transfer</name>
  <url>http://puton.info/article/spark-transfer.html</url>
  <developers>
    <developer>
      <id>taoyang</id>
      <email>ty@puton.info</email>
      <url>puton.info</url>
    </developer>
  </developers>

  <properties>

    <!-- base settings -->
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.build.jdk>1.7</project.build.jdk>

    <!-- lib versions -->
    <spark.version>1.3.1</spark.version>
    <!--<spark.version>1.6.2</spark.version>-->
    <hadoop.version>2.2.0</hadoop.version>
    <!--<hadoop.version>2.6.4</hadoop.version>-->
    <!--<hive.version>1.2.1</hive.version>-->

    <commons.io.version>2.4</commons.io.version>
    <commons.dbutils.version>1.6</commons.dbutils.version>
    <commons.lang3.version>3.4</commons.lang3.version>
    <gson.version>2.6.2</gson.version>
    <guava.version>19.0</guava.version>

    <junit.version>4.12</junit.version>
    <mysql.connector.version>5.1.38</mysql.connector.version>

    <!-- plugin versions -->
    <plugin.maven-compiler>3.5.1</plugin.maven-compiler>
    <plugin.maven-assembly>2.6</plugin.maven-assembly>
    <plugin.maven-surefire>2.19.1</plugin.maven-surefire>
    <skipTests>true</skipTests>

  </properties>

  <dependencies>

    <!-- hadoop start -->
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-core</artifactId>
      <version>1.2.1</version>
      <scope>${hadoop-core.scope}</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-common</artifactId>
      <version>${hadoop.version}</version>
      <scope>${hadoop.scope}</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-client</artifactId>
      <version>${hadoop.version}</version>
      <scope>${hadoop.scope}</scope>
    </dependency>
    <!-- hadoop end -->
    <!-- spark start -->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.10</artifactId>
      <version>${spark.version}</version>
      <scope>${spark.scope}</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_2.10</artifactId>
      <version>${spark.version}</version>
      <scope>${spark.scope}</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_2.10</artifactId>
      <version>${spark.version}</version>
      <scope>${spark.scope}</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_2.10</artifactId>
      <version>${spark.version}</version>
      <scope>${spark.scope}</scope>
    </dependency>
    <dependency>
      <groupId>com.databricks</groupId>
      <artifactId>spark-csv_2.10</artifactId>
      <version>1.4.0</version>
    </dependency>
    <!-- spark end -->

    <!-- log4j start -->
    <dependency>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
      <version>1.2.17</version>
    </dependency>
    <!-- log4j end -->

    <!-- junit start -->
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>${junit.version}</version>
      <scope>test</scope>
    </dependency>
    <!-- junit end -->

    <!--mysql start-->
    <dependency>
      <groupId>mysql</groupId>
      <artifactId>mysql-connector-java</artifactId>
      <version>${mysql.connector.version}</version>
    </dependency>
    <!--mysql end-->

    <!--teradata start-->
    <dependency>
      <groupId>info.puton.driver</groupId>
      <artifactId>terajdbc4</artifactId>
      <version>15.10.0.14</version>
    </dependency>
    <dependency>
      <groupId>info.puton.driver</groupId>
      <artifactId>tdgssconfig</artifactId>
      <version>15.10.0.14</version>
    </dependency>
    <!--teradata end-->

    <!--db2 start-->
    <dependency>
      <groupId>info.puton.driver</groupId>
      <artifactId>db2jcc</artifactId>
      <version>9.7</version>
    </dependency>
    <dependency>
      <groupId>info.puton.driver</groupId>
      <artifactId>db2jcc_license_cu</artifactId>
      <version>9.7</version>
    </dependency>
    <!--db2 end-->

    <dependency>
      <groupId>commons-io</groupId>
      <artifactId>commons-io</artifactId>
      <version>${commons.io.version}</version>
    </dependency>

    <dependency>
      <groupId>commons-dbutils</groupId>
      <artifactId>commons-dbutils</artifactId>
      <version>${commons.dbutils.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.commons</groupId>
      <artifactId>commons-lang3</artifactId>
      <version>${commons.lang3.version}</version>
    </dependency>

    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
      <version>${guava.version}</version>
    </dependency>

    <dependency>
      <groupId>com.google.code.gson</groupId>
      <artifactId>gson</artifactId>
      <version>${gson.version}</version>
    </dependency>

  </dependencies>

  <build>

    <sourceDirectory>src/main/java</sourceDirectory>
    <testSourceDirectory>src/test/java</testSourceDirectory>

    <!--配置Maven 对resource文件 过滤 -->
    <resources>
      <resource>
        <directory>src/main/resources</directory>
        <includes>
          <include>**/*.properties</include>
          <include>**/*.xml</include>
        </includes>
        <filtering>true</filtering>
      </resource>
      <resource>
        <directory>src/main/java</directory>
        <includes>
          <include>**/*.properties</include>
          <include>**/*.xml</include>
        </includes>
        <filtering>true</filtering>
      </resource>
    </resources>

    <plugins>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>${plugin.maven-compiler}</version>
        <configuration>
          <source>${project.build.jdk}</source>
          <target>${project.build.jdk}</target>
          <encoding>${project.build.sourceEncoding}</encoding>
        </configuration>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-surefire-plugin</artifactId>
        <version>${plugin.maven-surefire}</version>
        <configuration>
          <skipTests>${skipTests}</skipTests>
        </configuration>
      </plugin>

      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <version>${plugin.maven-assembly}</version>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
        </configuration>
        <executions>
          <execution>
            <id>make-jar</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

    </plugins>
  </build>

  <profiles>

    <profile>
      <id>develop</id>
      <properties>
        <spark.scope>compile</spark.scope>
        <hadoop-core.scope>compile</hadoop-core.scope>
        <hadoop.scope>compile</hadoop.scope>
        <hive.scope>compile</hive.scope>
        <spark.mode>local</spark.mode>
        <datasource.config.path>D:/programming/java/component/spark-transfer/src/main/resources/spark-transfer/datasource.properties</datasource.config.path>
      </properties>

      <activation>
        <activeByDefault>true</activeByDefault>
      </activation>

    </profile>

    <profile>
      <id>vm</id>
      <properties>
        <spark.scope>provided</spark.scope>
        <hadoop-core.scope>provided</hadoop-core.scope>
        <hadoop.scope>provided</hadoop.scope>
        <hive.scope>provided</hive.scope>
        <spark.mode>vmcluster</spark.mode>
        <datasource.config.path>/home/hadoop/app/spark-transfer/datasource.properties</datasource.config.path>
      </properties>

      <!--<activation>-->
      <!--<activeByDefault>true</activeByDefault>-->
      <!--</activation>-->

    </profile>

    <profile>
      <id>test</id>
      <properties>
        <spark.scope>provided</spark.scope>
        <hadoop.scope>provided</hadoop.scope>
        <hadoop-core.scope>provided</hadoop-core.scope>
        <hive.scope>provided</hive.scope>
        <spark.mode>testcluster</spark.mode>
        <datasource.config.path>/home/tdtest/td/spark-transfer/datasource.properties</datasource.config.path>
      </properties>

      <!--<activation>-->
      <!--<activeByDefault>true</activeByDefault>-->
      <!--</activation>-->

    </profile>

    <profile>
      <id>production</id>
      <properties>
        <spark.scope>provided</spark.scope>
        <hadoop.scope>provided</hadoop.scope>
        <hadoop-core.scope>provided</hadoop-core.scope>
        <hive.scope>provided</hive.scope>
        <spark.mode>prodcluster</spark.mode>
        <datasource.config.path>/home/xytd/td/spark-transfer/datasource.properties</datasource.config.path>
      </properties>

      <!--<activation>-->
      <!--<activeByDefault>true</activeByDefault>-->
      <!--</activation>-->

    </profile>

  </profiles>

</project>
